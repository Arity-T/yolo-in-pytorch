{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arity-T/yolo-in-pytorch/blob/main/train_yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwJrDRl23qWe"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL14I0pc3qWh"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/Arity-T/yolo-in-pytorch.git\n",
        "%cd /content/yolo-in-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FnAHzX8__or"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaZ46NMf3qWi"
      },
      "source": [
        "## VOC dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Quxbld_dCM"
      },
      "source": [
        "Download from pjreddie.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOAYxaeo3qWj"
      },
      "outputs": [],
      "source": [
        "# !wget https://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar\n",
        "# !wget https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
        "# !wget https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkylCGX5_io2"
      },
      "source": [
        "Or copy from Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCXmmYE8_qE2"
      },
      "outputs": [],
      "source": [
        "drive_voc_path = \"/content/drive/MyDrive/Projects/YOLO/VOC\"\n",
        "!ls \"$drive_voc_path\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfYbWHxu_ron"
      },
      "outputs": [],
      "source": [
        "!cp \"$drive_voc_path/VOCtrainval_11-May-2012.tar\" .\n",
        "!cp \"$drive_voc_path/VOCtrainval_06-Nov-2007.tar\" .\n",
        "!cp \"$drive_voc_path/VOCtest_06-Nov-2007.tar\" ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwbnta-6ASKM"
      },
      "source": [
        "Extract and convert labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaXvHgzW_Gmz"
      },
      "outputs": [],
      "source": [
        "!tar xf VOCtrainval_11-May-2012.tar\n",
        "!tar xf VOCtrainval_06-Nov-2007.tar\n",
        "!tar xf VOCtest_06-Nov-2007.tar\n",
        "!python convert_voc_labels.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9kzZFav3qWk"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9FSTBhD3qWl"
      },
      "outputs": [],
      "source": [
        "import multiprocessing as mp\n",
        "\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "import utils\n",
        "import yolov1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d7zlBsF3qWl"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUmzP258PC5z"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYX3djAgPGuQ"
      },
      "source": [
        "Create model with pretrained backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpLzHwq53qWm"
      },
      "outputs": [],
      "source": [
        "# from torchvision.models import resnet34\n",
        "\n",
        "# model = yolov1.Model(backbone=resnet34(weights=\"DEFAULT\")).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN4ZojXBOSDz"
      },
      "source": [
        "Or load weights from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN5tZmnIOSX0"
      },
      "outputs": [],
      "source": [
        "drive_weights_path = \"/content/drive/MyDrive/Projects/YOLO/model_loss_1.05.pth\"\n",
        "\n",
        "model = torch.load(drive_weights_path, map_location=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlHYXLvL3qWn"
      },
      "source": [
        "## Datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmJos4jI3qWn"
      },
      "outputs": [],
      "source": [
        "# List of augmentations\n",
        "augs = A.Compose(\n",
        "    [\n",
        "        A.Resize(448, 448, always_apply=True),\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(format=\"yolo\"),\n",
        ")\n",
        "trfs = torchvision.transforms.ToTensor()\n",
        "\n",
        "# Datasets\n",
        "train_ds = yolov1.Dataset(\n",
        "    img_sets=[\n",
        "        \"voc2007_train.txt\",\n",
        "        \"voc2007_val.txt\",\n",
        "        \"voc2007_test.txt\",\n",
        "        \"voc2012_train.txt\",\n",
        "    ],\n",
        "    augmentations=augs,\n",
        "    transforms=trfs,\n",
        ")\n",
        "val_ds = yolov1.Dataset(\n",
        "    img_sets=[\"voc2012_val.txt\"], augmentations=augs, transforms=trfs\n",
        ")\n",
        "\n",
        "# Dataloaders\n",
        "print(\"\\nCPU count:\", mp.cpu_count())\n",
        "train_dl = DataLoader(\n",
        "    train_ds, batch_size=64, collate_fn=yolov1.collate_fn, num_workers=mp.cpu_count()\n",
        ")\n",
        "val_dl = DataLoader(\n",
        "    val_ds, batch_size=64, collate_fn=yolov1.collate_fn, num_workers=mp.cpu_count()\n",
        ")\n",
        "\n",
        "labels = open(\"voc_classes.txt\").read().split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UweuyDoc3qWp"
      },
      "outputs": [],
      "source": [
        "imgs, annots = next(iter(train_dl))\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(utils.draw_bboxes(imgs[0], annots[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNUr0cQUQVFE"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoUNCNljQlGX"
      },
      "source": [
        "It is better to freeze the layers if a pretrained backbone is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZpwTs0jWxlM"
      },
      "outputs": [],
      "source": [
        "# Freeze all backbone layers\n",
        "model.backbone.requires_grad_(False)\n",
        "\n",
        "# And then unfreeze only a few last layers\n",
        "model.backbone[0][-1].requires_grad_(True)\n",
        "model.backbone[1].requires_grad_(True)\n",
        "model.backbone[2].requires_grad_(True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF4LSAFlWvcY"
      },
      "outputs": [],
      "source": [
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NOAU36T3qWp"
      },
      "outputs": [],
      "source": [
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.0005, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "loss_fn = yolov1.Loss(labmda_coord=5.0, labmda_noobj=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh2WfqV-3qWq"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    mean_loss = []\n",
        "    for imgs, annots in tqdm(train_dl, leave=False):\n",
        "        predicticted_grids = model(imgs.to(device))\n",
        "        loss = loss_fn(predicticted_grids, annots)\n",
        "        mean_loss.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"\\nMean loss:\", sum(mean_loss) / len(mean_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cak0iQKBnzac"
      },
      "outputs": [],
      "source": [
        "# Best loss: 0.83"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0Iv1LJ_QPBU"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBKZJSG5kS_k"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/Projects/YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMqlO5F4kcTv"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"/content/drive/MyDrive/Projects/YOLO/model_last.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIkUEXzhYyIT"
      },
      "source": [
        "## Compute metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Atdm8oKbYF90"
      },
      "outputs": [],
      "source": [
        "train_annots = []\n",
        "train_predictions = []\n",
        "\n",
        "for imgs, annots in tqdm(train_dl, leave=False):\n",
        "    train_annots += annots\n",
        "    train_predictions += model.predict(\n",
        "        imgs.to(device), threshold=0.25, iou_threshold=0.5\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "800tLbvrelbM"
      },
      "outputs": [],
      "source": [
        "train_map, train_map_by_classes = utils.compute_map(\n",
        "    train_predictions, train_annots, iou_threshold=0.5, number_of_classes=20\n",
        ")\n",
        "print(\"MAP@0.5 on train:\", train_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPpH6Mj1bfo2"
      },
      "outputs": [],
      "source": [
        "val_annots = []\n",
        "val_predictions = []\n",
        "\n",
        "for imgs, annots in tqdm(val_dl, leave=False):\n",
        "    val_annots += annots\n",
        "    val_predictions += model.predict(imgs.to(device), threshold=0.25, iou_threshold=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx16Gr6hexV1"
      },
      "outputs": [],
      "source": [
        "val_map, val_map_by_classes = utils.compute_map(\n",
        "    val_predictions, val_annots, iou_threshold=0.5, number_of_classes=20\n",
        ")\n",
        "print(\"MAP@0.5 on val:\", val_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9w8bGDcSKRE"
      },
      "source": [
        "## Some examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmV1RyD8VsHH"
      },
      "outputs": [],
      "source": [
        "# Take images from training set\n",
        "imgs_train, annots_train = next(iter(train_dl))\n",
        "print(\"Batch size:\", len(imgs_train))\n",
        "\n",
        "example_indexes = range(10)\n",
        "\n",
        "utils.show_examples(\n",
        "    imgs_train[example_indexes],\n",
        "    model.predict(imgs_train[example_indexes].to(device), threshold=0.2),\n",
        "    [annots_train[i] for i in example_indexes],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI8Efj6EWE2U"
      },
      "outputs": [],
      "source": [
        "# Take images from validation set\n",
        "imgs_test, annots_test = next(iter(val_dl))\n",
        "print(\"Batch size:\", len(imgs_test))\n",
        "\n",
        "example_indexes = range(10)\n",
        "\n",
        "utils.show_examples(\n",
        "    imgs_test[example_indexes],\n",
        "    model.predict(imgs_test[example_indexes].to(device), threshold=0.2),\n",
        "    [annots_test[i] for i in example_indexes],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vo_VfheQRJ0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "2b1a23a12ef12311a32e2b7719cdb51ed92526f3789a09c4963ee17dd8abb4f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}